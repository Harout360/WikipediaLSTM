{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "eminem\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-f2c2ca95685d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m     \u001b[0mcall_scrape_artist_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;31m# Remove some non-lyrics lines from the page and save to txt / zip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-f2c2ca95685d>\u001b[0m in \u001b[0;36mcall_scrape_artist_function\u001b[1;34m(artist)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;31m# We scrape the main page as well as 4 subsequent pages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[0mscrape_artist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://www.metrolyrics.com/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'-lyrics.html'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mscrape_artist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://www.metrolyrics.com/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'-alpage-'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.html'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-f2c2ca95685d>\u001b[0m in \u001b[0;36mscrape_artist\u001b[1;34m(current_link, artist)\u001b[0m\n\u001b[0;32m     78\u001b[0m                             \u001b[0mcsvFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[0mlink_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_links_from_main_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_link\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;31m# Get the link list and write all lyrics to csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-f2c2ca95685d>\u001b[0m in \u001b[0;36mget_links_from_main_page\u001b[1;34m(current_link, artist)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_links\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lyrics-'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'orrection'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m                 \u001b[0mlink_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlink_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "#import the Beautiful soup functions to parse the data returned from the website\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import urllib.request as urllib2\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "linecount = 0\n",
    "visited = []\n",
    "\n",
    "# Define a function that scrapes lyrics from metro lyrics for a certain artist\n",
    "# The main page for the artist should be fed, as well as the artist's name\n",
    "# The function collects all song links from the page and saves their text to csv\n",
    "\n",
    "def scrape_artist(current_link, artist):\n",
    "       \n",
    "    def get_links_from_main_page(current_link, artist):\n",
    "    \n",
    "        #Query the website and return the html to the variable 'page'\n",
    "        page = urllib2.urlopen(current_link)\n",
    "            \n",
    "        #Parse the html in the 'page' variable, and store it in Beautiful Soup format\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "            \n",
    "        # get all links\n",
    "        all_links=soup.find_all('a')\n",
    "        \n",
    "        # Append useful links to link_list\n",
    "        link_list = []\n",
    "        for link in all_links:\n",
    "            url = link.get('href')\n",
    "            \n",
    "            url= str(url).encode('utf-8')\n",
    "            if url.find('lyrics-'+artist) > -1 and url.find('orrection') == -1:\n",
    "                link_list.append(url)\n",
    "        return link_list\n",
    "     \n",
    "    # Clean a single line of lyrics\n",
    "    # Only lowercase letters and white space is kept\n",
    "    def clean_line(line):\n",
    "        line_cleaned = ''\n",
    "        for letter in line:\n",
    "            if letter.lower() in string.ascii_lowercase or letter == ' ' :\n",
    "                line_cleaned = line_cleaned + letter.lower()\n",
    "                \n",
    "        if len(line_cleaned.split(' ')) > 3:\n",
    "            return line_cleaned\n",
    "        else:\n",
    "            return ''\n",
    "            \n",
    "    # Write lyrics to a csv file\n",
    "    def write_lyrics_to_csv(current_link):\n",
    "        global linecount, visited\n",
    "        \n",
    "        # Double links sometimes occur. We discard them.                \n",
    "        if not current_link in visited:\n",
    "            \n",
    "            visited.append(current_link)\n",
    "            \n",
    "            #Query the website and return the html to the variable 'page'\n",
    "            page = urllib2.urlopen(current_link)\n",
    "                \n",
    "            #Parse the html in the 'page' variable, and store it in Beautiful Soup format\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "            \n",
    "            # get all text\n",
    "            l = soup.find_all('p')\n",
    "                \n",
    "            for index,value in enumerate(l):\n",
    "                \n",
    "                if index > 4:\n",
    "                    paragraph = l[index].text\n",
    "                    for line in paragraph.split('\\n'):\n",
    "                            line_cleaned = clean_line(line)                                  \n",
    "                                        \n",
    "                            csvWriter.writerow([line_cleaned])\n",
    "                            linecount += 1\n",
    "                            csvFile.flush()\n",
    "            \n",
    "    link_list = get_links_from_main_page(current_link, artist)\n",
    "    \n",
    "    # Get the link list and write all lyrics to csv\n",
    "    for link in link_list:\n",
    "        write_lyrics_to_csv(link)\n",
    "        \n",
    "\n",
    "        \n",
    "# Create CSV-file for writing\n",
    "csvFile = open('eminem.csv', 'a')\n",
    "\n",
    "#Use csv Writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "\n",
    "# Scrape metro lyrics for a list of artists\n",
    "artist_list = ['eminem']#, 'dr-dre', 'snoop-dogg', '50-cent', 'jay-z', 'd-12','xzibit',\n",
    "               #'nate-dogg', 'lil-wayne', 'kanye-west', 'nas', '2pac', 'notorious-big']\n",
    "\n",
    "def call_scrape_artist_function(artist):\n",
    "    \n",
    "    # We scrape the main page as well as 4 subsequent pages\n",
    "    scrape_artist('http://www.metrolyrics.com/'+artist+'-lyrics.html', artist)\n",
    "    for i in range(4):        \n",
    "        scrape_artist('http://www.metrolyrics.com/'+artist+'-alpage-'+str(i)+'.html', artist)\n",
    "        \n",
    "# We now do this for each artist on the list\n",
    "for artist in tqdm.tqdm(artist_list):\n",
    "    print ('\\n')\n",
    "    print (artist)\n",
    "    print ('\\n')\n",
    "    call_scrape_artist_function(artist)\n",
    "\n",
    "# Remove some non-lyrics lines from the page and save to txt / zip\n",
    "with open('eminem.txt', 'w') as f1:\n",
    "    with open('eminem.csv') as f2:\n",
    "        for line in f2:\n",
    "             if line.find('song') == -1 and line.find('lyrics') == -1 and len(line)>5:\n",
    "                 f1.write(line)\n",
    "    f1.close()\n",
    "\n",
    "csvFile.close()\n",
    "\n",
    "with zipfile.ZipFile('eminem.zip', 'w') as myzip:\n",
    "    myzip.write('eminem.txt')\n",
    "    myzip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
