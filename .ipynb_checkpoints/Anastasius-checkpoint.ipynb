{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 🤴Anastasius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": null
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing import text as Text\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#  Declare  text source and constants\n",
    "It is important to not read the entire file into memory.\n",
    "\n",
    "The file is far too large to keep in memory.\n",
    "\n",
    "Instead we read portions into memory, vectorize it, pass\n",
    "that data into the lstm, and repeat until we process the entire file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": null
   },
   "outputs": [],
   "source": [
    "data_source = 'wikitext-103-raw/wiki.test.raw'\n",
    "\n",
    "# Determine file size in bytes\n",
    "file_size = os.path.getsize(data_source)\n",
    "\n",
    "# Percentage of file to read into memory\n",
    "data_slice_percentage = .1\n",
    "\n",
    "# Total number of slices\n",
    "total_slices = 1 / data_slice_percentage\n",
    "\n",
    "# WTF AM I DOING!\n",
    "data_slice_in_chars = math.ceil(file_size * data_slice_percentage) \n",
    "\n",
    "# How much of a sentence chunk to put into the LSTM\n",
    "chunk_size = 50\n",
    "skip = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create text generator\n",
    "This method returns an ***iterator*** over the file that outputs a string of length **data_slice_size**. Every call to this method returns a different iterator starting from the beginning of the file.\n",
    "\n",
    "The text returned is not processed, so we must vectorize the data, and run the data through the LSTM. \n",
    "\n",
    "**Note:** Mohammed said this chunk size should be just large enough to fit into the gpu, if the size is too large or too small then the application spends more time transfering the data from CPU -> GPU than actually processing it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text_iter():\n",
    "    with open(data_source, encoding='utf8') as file:\n",
    "        while True:\n",
    "            chunk = list(file.read(data_slice_in_chars))\n",
    "            if chunk:\n",
    "                yield chunk\n",
    "            else:\n",
    "                return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine $\\sum$\n",
    "We need to know all the possible characters in the file. Each of the characters is a class that the network can predict. We find $\\sum$ before doing any heavy operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding sigma of slice  1  of 10.0\n",
      "Finding sigma of slice  2  of 10.0\n",
      "Finding sigma of slice  3  of 10.0\n",
      "Finding sigma of slice  4  of 10.0\n",
      "Finding sigma of slice  5  of 10.0\n",
      "Finding sigma of slice  6  of 10.0\n",
      "Finding sigma of slice  7  of 10.0\n",
      "Finding sigma of slice  8  of 10.0\n",
      "Finding sigma of slice  9  of 10.0\n",
      "Finding sigma of slice  10  of 10.0\n",
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '^', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', '¥', '©', '°', '½', 'Á', 'Æ', 'É', '×', 'ß', 'à', 'á', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'í', 'î', 'ñ', 'ó', 'ô', 'ö', 'ú', 'ü', 'ć', 'č', 'ě', 'ī', 'ł', 'Ō', 'ō', 'Š', 'ū', 'ž', 'ǐ', 'ǔ', 'ǜ', 'ə', 'ɛ', 'ɪ', 'ʊ', 'ˈ', 'ː', '̍', '͘', 'Π', 'Ω', 'έ', 'α', 'β', 'δ', 'ε', 'ι', 'λ', 'μ', 'ν', 'ο', 'π', 'ς', 'σ', 'τ', 'υ', 'ω', 'ό', 'П', 'в', 'д', 'и', 'к', 'н', 'א', 'ב', 'י', 'ל', 'ר', 'ש', 'ת', 'ا', 'ت', 'د', 'س', 'ك', 'ل', 'و', 'ڠ', 'ग', 'न', 'र', 'ल', 'ष', 'ु', 'े', 'ो', '्', 'ả', 'ẩ', '‑', '–', '—', '’', '“', '”', '†', '‡', '…', '⁄', '₩', '₱', '→', '−', '♯', 'の', 'ア', 'イ', 'ク', 'グ', 'ジ', 'ダ', 'ッ', 'ド', 'ナ', 'ブ', 'ラ', 'ル', '中', '为', '伊', '傳', '八', '利', '前', '勢', '史', '型', '士', '大', '学', '宝', '开', '律', '成', '戦', '春', '智', '望', '杜', '東', '民', '王', '甫', '田', '甲', '秘', '聖', '艦', '處', '衛', '解', '詩', '贈', '邵', '都', '鉄', '集', '魯']\n"
     ]
    }
   ],
   "source": [
    "# Slice number\n",
    "slice_number = 1\n",
    "\n",
    "vocab = set()\n",
    "for data_slice in text_iter():\n",
    "    print(\"Finding sigma of slice \", slice_number, ' of', total_slices)\n",
    "    for letter in data_slice:\n",
    "        vocab.add(letter)\n",
    "    slice_number += 1\n",
    "\n",
    "vocab = sorted(list(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize $\\sum$\n",
    "We cannot directly pass characters into the LSTM, we must pass in numerical data. \n",
    "\n",
    "We map letter -> integer based on their **position** in $\\sum$. We pass this integer into the LSTM which will output another integer in the range of $\\sum$. We then map this integer -> letter for generating the next character in the sequence\n",
    "\n",
    "We also create a one hot encoded version of each integer, that is suppose $|\\sum| = n$ and $a \\in \\sum$ and encoder[a] = 1. Then the one hot encoding of a would be [0,1,0,.....(nth 0)]\n",
    "\n",
    "Needs more explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Enocder maps a letter -> integer\n",
    "encoder = dict((letter,position) for position, letter in enumerate(vocab))\n",
    "\n",
    "# Decoder maps an integer -> letter\n",
    "decoder = dict((value,key) for key,value in encoder.items())\n",
    "\n",
    "# Creates one hot encoding from 0.... vocab length\n",
    "onehot = np.eye(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train LSTM\n",
    "We read in slices from the file, vectorize the slice, form our training set, and pass this set into our LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing Slice  1  of  10.0 ...    ✓\n",
      "Vectorizing Slice  2  of  10.0 ...    ✓\n",
      "Vectorizing Slice  3  of  10.0 ...    ✓\n",
      "Vectorizing Slice  4  of  10.0 ...    ✓\n",
      "Vectorizing Slice  5  of  10.0 ...    ✓\n",
      "Vectorizing Slice  6  of  10.0 ...    ✓\n",
      "Vectorizing Slice  7  of  10.0 ...    ✓\n",
      "Vectorizing Slice  8  of  10.0 ...    ✓\n",
      "Vectorizing Slice  9  of  10.0 ...    ✓\n",
      "Vectorizing Slice  10  of  10.0 ...    ✓\n"
     ]
    }
   ],
   "source": [
    "# Slice number\n",
    "slice_number = 1\n",
    "\n",
    "# Reads in a single slice per loop iteration\n",
    "for data_slice in text_iter():\n",
    "    # One hot the data slice (dont enumerate, too large to create tuples)\n",
    "    print(\"Processing Slice \", slice_number, \" of \", total_slices,'...', end='')\n",
    "    i = 0\n",
    "    slice_number += 1\n",
    "    print('V',end='')\n",
    "    for letter in data_slice:\n",
    "        data_slice[i] = onehot[encoder[letter]]\n",
    "        i += 1\n",
    "    print(u'\\u2713','...CI',end='')\n",
    "    # Get the starting location of all text chunks\n",
    "    chunk_indices = range(0, len(data_slice) - chunk_size, skip)\n",
    "    \n",
    "    print(u'\\u2713','...X',end='')\n",
    "    # Take chunk sized lengths from the data vector\n",
    "    X = [data[ i : i + chunk_size] for i in chunk_indices]\n",
    "\n",
    "    print(u'\\u2713','...Y',end='')\n",
    "    # Store the next letter after the chunk sized length\n",
    "    Y = [data[ i + chunk_size] for i in chunk_indices]\n",
    "    \n",
    "    print(u'\\u2713')\n",
    "    # Data slice is now vectorized\n",
    "    # Use it or lose it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Meta-Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "executionInfo": null
   },
   "outputs": [],
   "source": [
    "# The unique ascii chars within the data set\n",
    "uniqueLetters = sorted(list(set(data)))\n",
    "print(uniqueLetters)\n",
    "\n",
    "# Enocder maps a letter -> integer\n",
    "encoder = dict((letter,letterPosition) for letterPosition, letter in enumerate(uniqueLetters))\n",
    "\n",
    "# Decoder maps an integer -> letter\n",
    "decoder = dict((value,key) for key,value in encoder.items())\n",
    "\n",
    "# Text chunk represents the amount of text to feed into the network\n",
    "text_chunk_len = 50\n",
    "\n",
    "# Skip represents how far we 'slide' down the text file per chunk\n",
    "skip = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "executionInfo": null
   },
   "outputs": [],
   "source": [
    "# One hot encode the unique letters\n",
    "onehot = np.eye(len(uniqueLetters), dtype=bool)\n",
    "print(onehot)\n",
    "\n",
    "# Transform char -> int -> onehot\n",
    "for i, letter in enumerate(data):\n",
    "  data[i] = onehot[encoder[data[i]]]\n",
    "  \n",
    "# Get the starting location of all text chunks\n",
    "chunk_indices = range(0, len(data_vector) - text_chunk_len, skip)\n",
    "\n",
    "# Take chunk sized lengths from the data vector\n",
    "X = [data[ i : i + text_chunk_len] for i in chunk_indices]\n",
    "\n",
    "# Store the next letter after the chunk sized length\n",
    "Y = [data[ i + text_chunk_len] for i in chunk_indices]\n",
    "\n",
    "print(\"Finished vectorizing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "executionInfo": null
   },
   "outputs": [],
   "source": [
    "#Build the model using Keras\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(text_chunk_len, len(uniqueLetters))))\n",
    "model.add(Dense(len(uniqueLetters)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "print(\"Finished building model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##  Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "executionInfo": null
   },
   "outputs": [],
   "source": [
    "#Train the model\n",
    "\n",
    "#Save weights to checkpoints file during training\n",
    "chkpt_filepath = \"/checkpoint/weights.best.hdf5\"\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(chkpt_filepath,\n",
    "                                               monitor='val_loss',\n",
    "                                               verbose=0,\n",
    "                                               save_best_only=False,\n",
    "                                               save_weights_only=False,\n",
    "                                               mode='auto',\n",
    "                                               period=1)\n",
    "\n",
    "for iteration in range(1, 60):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, Y,\n",
    "              batch_size=128,\n",
    "              epochs=1,\n",
    "              callbacks=[checkpointer]) #save model\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "executionInfo": null
   },
   "outputs": [],
   "source": [
    "# Load saved weights and recreate model\n",
    "model = Sequential()\n",
    "model.load_weights(\"/checkpoint/weights.best.hdf5\")\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "# get random seed sentence from raw_data\n",
    "start_index = random.randint(0, len(raw_data) - text_chunk_len - 1)\n",
    "\n",
    "print('\\n----- diversity:', diversity)\n",
    "\n",
    "generated = ''\n",
    "sentence = text[start_index: start_index + text_chunk_len]\n",
    "generated += sentence\n",
    "print('----- Generating with seed: \"' + sentence + '\"')\n",
    "sys.stdout.write(generated)\n",
    "\n",
    "for i in range(400):\n",
    "  x = np.zeros((1, len_per_section, char_size))\n",
    "  for t, char in enumerate(sentence):\n",
    "    x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    next_index = sample(preds, diversity)\n",
    "    next_char = indices_char[next_index]\n",
    "\n",
    "    generated += next_char\n",
    "    sentence = sentence[1:] + next_char\n",
    "\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "colabVersion": "0.1",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
