{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ðŸ¤´Anastasius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": null
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing import text as Text\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#  Declare  text source and constants\n",
    "It is important to not read the entire file into memory.\n",
    "\n",
    "The file is far too large to keep in memory.\n",
    "\n",
    "Instead we read portions into memory, vectorize it, pass\n",
    "that data into the lstm, and repeat until we process the entire file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": null
   },
   "outputs": [],
   "source": [
    "# Input sources\n",
    "data_source = 'wikitext-103-raw/wiki.test.vector'\n",
    "vocab_path = 'wikitext-103-raw/wiki.test.vocab'\n",
    "decoder_path = 'wikitext-103-raw/wiki.test.decoder.npy'\n",
    "data_len_path = 'wikitext-103-raw/wiki.test.vector.len'\n",
    "\n",
    "\n",
    "# How many rows to read in from the csv\n",
    "chunk_size = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Load Processed data\n",
    "We need to know all the possible characters in the file. Each of the characters is a class that the network can predict. We find $\\sum$ before doing any heavy operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB:\n",
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~']\n",
      "\n",
      "DECODER:\n",
      "{0: 'Z', 1: '\\n', 2: 'g', 3: '7', 4: 'L', 5: '|', 6: 'i', 7: '&', 8: 'f', 9: 'U', 10: 'C', 11: 'q', 12: 'c', 13: '+', 14: '-', 15: 'o', 16: '5', 17: 'V', 18: 'e', 19: ')', 20: '4', 21: ';', 22: 'Y', 23: '1', 24: '`', 25: 'z', 26: '[', 27: 't', 28: '(', 29: '!', 30: 's', 31: 'W', 32: '<', 33: '^', 34: 'G', 35: '}', 36: ' ', 37: '8', 38: 'N', 39: '_', 40: 'p', 41: \"'\", 42: 'b', 43: 'P', 44: 'B', 45: 'a', 46: '.', 47: '/', 48: 'm', 49: 'H', 50: 'u', 51: '~', 52: 'O', 53: 'T', 54: 'J', 55: '{', 56: 'x', 57: '3', 58: '9', 59: ':', 60: 'F', 61: '2', 62: ']', 63: '0', 64: 'j', 65: 'K', 66: '?', 67: 'X', 68: '=', 69: 'Q', 70: 'E', 71: 'M', 72: 'v', 73: '6', 74: '$', 75: 'd', 76: 'h', 77: '%', 78: 'R', 79: '\\\\', 80: 'n', 81: 'r', 82: '#', 83: 'y', 84: '>', 85: 'l', 86: ',', 87: 'S', 88: 'k', 89: 'w', 90: '\"', 91: 'I', 92: '*', 93: 'D', 94: 'A'}\n",
      "\n",
      "ENCODER:\n",
      "{'\"': 90, 'J': 54, '{': 55, \"'\": 41, '(': 28, 'h': 76, 'B': 44, '8': 37, '4': 20, '!': 29, '#': 82, '1': 23, 'O': 52, '`': 24, 'C': 10, 'Y': 22, 'I': 91, 'b': 42, 'f': 8, '\\n': 1, '9': 58, '2': 61, '-': 14, 'P': 43, ';': 21, ':': 59, 'n': 80, 'S': 87, 'V': 17, '?': 66, '~': 51, 'E': 70, 'z': 25, 'U': 9, '5': 16, 'W': 31, 'a': 45, 'R': 78, 'r': 81, '+': 13, 'G': 34, '/': 47, '$': 74, 'T': 53, '_': 39, 'o': 15, 'k': 88, 'j': 64, 'q': 11, 'i': 6, 'D': 93, '.': 46, '6': 73, 'M': 71, 's': 30, 'X': 67, 'N': 38, '>': 84, 'Q': 69, '3': 57, 'm': 48, '=': 68, '&': 7, 'L': 4, '^': 33, 'F': 60, '<': 32, '%': 77, '[': 26, 'w': 89, 'l': 85, '7': 3, 'g': 2, 'u': 50, 'd': 75, 'A': 94, 'p': 40, 'y': 83, 'c': 12, 'e': 18, 'v': 72, '}': 35, '0': 63, 't': 27, ']': 62, '\\\\': 79, 'x': 56, ',': 86, 'H': 49, '|': 5, '*': 92, 'Z': 0, ')': 19, 'K': 65, ' ': 36}\n"
     ]
    }
   ],
   "source": [
    "# Load Vocab\n",
    "with open(vocab_path,\"r\") as file:\n",
    "    vocab = sorted(list(file.read()))\n",
    "\n",
    "# Load Decoder\n",
    "decoder = np.load(decoder_path).item()\n",
    "\n",
    "# Build Encoder\n",
    "encoder = dict((value,key) for key,value in decoder.items())\n",
    "\n",
    "\n",
    "# Load CSV Row count\n",
    "with open(data_len_path,\"r\") as file:\n",
    "    data_row_count = int(file.read())\n",
    "    \n",
    "print(\"VOCAB:\")\n",
    "print(vocab)\n",
    "\n",
    "print(\"\\nDECODER:\")\n",
    "print(decoder)\n",
    "\n",
    "print(\"\\nENCODER:\")\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# One hot encode vocab\n",
    "Passing in raw numbers into an LSTM is apparently a bad idea. We should be passing in one hot encoded values because it does not introduce 'order' or 'weight' for each letter that we don't want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True False False ..., False False False]\n",
      " [False  True False ..., False False False]\n",
      " [False False  True ..., False False False]\n",
      " ..., \n",
      " [False False False ...,  True False False]\n",
      " [False False False ..., False  True False]\n",
      " [False False False ..., False False  True]]\n"
     ]
    }
   ],
   "source": [
    "# Creates one hot encoding from 0.... vocab length\n",
    "onehot = np.eye(len(vocab),dtype=bool)\n",
    "print(onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Construct LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using saved model\n"
     ]
    }
   ],
   "source": [
    "# X Dimension = (Total sentences) x (Sentence Length) x (Character Vector dimension\n",
    "try:\n",
    "    model = load_model(\"ckpt/model.h5py\")\n",
    "    print(\"Using saved model\")\n",
    "except(OSError):\n",
    "    model = Sequential()\n",
    "    input_shape = (50, len(vocab))\n",
    "\n",
    "    model.add(LSTM(64, input_shape=input_shape))\n",
    "    model.add(Dense(len(vocab)))\n",
    "    model.add(Activation('softmax'))\n",
    "    print(\"Using new model\")\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train LSTM\n",
    "We read in slices from the file, vectorize the slice, form our training set, and pass this set into our LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1  of  2\n",
      "Training Model on chunk 1  of  2\n",
      "Epoch 1/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 3.2956Epoch 00000: loss improved from inf to 3.29524, saving model to ckpt/model.h5py\n",
      "100000/100000 [==============================] - 114s - loss: 3.2952   \n",
      "Epoch 2/5\n",
      " 59264/100000 [================>.............] - ETA: 45s - loss: 2.2669"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-87662b2f2e9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# Train model on X and Y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training Model on chunk\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_processed\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" of \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_chunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mchunk_processed\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\gregl\\Anaconda3\\envs\\intel\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    843\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 845\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    846\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\Users\\gregl\\Anaconda3\\envs\\intel\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1485\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1487\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\gregl\\Anaconda3\\envs\\intel\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1140\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1141\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\gregl\\Anaconda3\\envs\\intel\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2073\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2074\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\gregl\\Anaconda3\\envs\\intel\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\gregl\\Anaconda3\\envs\\intel\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\gregl\\Anaconda3\\envs\\intel\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\gregl\\Anaconda3\\envs\\intel\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\gregl\\Anaconda3\\envs\\intel\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_chunks = math.ceil(data_row_count / chunk_size)\n",
    "chunk_processed = 0\n",
    "\n",
    "# Saves entire model, tracks loss, save after each epoc \n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath=\"ckpt/model.h5py\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    monitor='loss',\n",
    "    mode='auto',\n",
    "    period=1)\n",
    "\n",
    "callbacks = [checkpointer]\n",
    "\n",
    "# Reads in chunk_size amount of rows from csv\n",
    "for file_epocs in range(10):\n",
    "    for chunk in pd.read_csv(data_source, chunksize=chunk_size, sep=';', dtype={'Y':np.int8}):\n",
    "        # Get input vectors ( Convert X to numpy array \n",
    "        print(\"Processing chunk\", chunk_processed + 1,\" of \", total_chunks)\n",
    "        rawX = chunk['X'].map(lambda x: np.fromstring(x, dtype=np.uint8, sep=\" \")).values\n",
    "        rawY = chunk['Y'].values\n",
    "\n",
    "        X = np.zeros(shape=(chunk_size, len(rawX[0]),len(vocab)),dtype=bool)\n",
    "        Y = np.zeros((chunk_size, len(vocab)), dtype=np.bool)\n",
    "\n",
    "        for i in range(len(rawX)):\n",
    "            X[i] = [onehot[value] for value in rawX[i]]\n",
    "            Y[i] = onehot[rawY[i]]\n",
    "\n",
    "        # Train model on X and Y\n",
    "        print(\"Training Model on chunk\", chunk_processed + 1,\" of \", total_chunks)\n",
    "        model.fit(X, Y, batch_size=128, epochs=5, verbose=1,callbacks=callbacks)\n",
    "        print(\"\\n\")\n",
    "        chunk_processed += 1\n",
    "\n",
    "\n",
    "print(\"Model Training Completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "\n",
    "    # helper function to sample an index from a probability array\n",
    "\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "\n",
    "    preds = np.log(preds) / temperature\n",
    "\n",
    "    exp_preds = np.exp(preds)\n",
    "\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, this text needs to be at MIN 50 characters HELLOOOOO.aeoaattttl s nlnieaaaataaeansn nnid sn   tait thataihlneahet tn i s i ithaansaanan  tl   n n aoheaoaiaaann   nnnnnn  aoaaaiieitannnnnnnsn r        itetoaaaaaanh   tnnnnn  aaahaaaioaannn nnnnnnn   aioiiiciaaansnnnn o  n      aftheaoaa,ant   nnnn    taaaaiitaoahlntnnnelnn   ia dde  taannaeentohlt  ts   r  itoheaaaetonh   cnn  n  aaaoooaoattnnnnnnnnn eo i iadidtrttnann ne aeaat  taran nneaahlel aiasln   etnnn e toax   eaath tiaoarrn  ahmnnn   aania   aaant cnaoonts  aoetnn   aann e  aaann  txtantn "
     ]
    }
   ],
   "source": [
    "sentence_raw = \"Hello, this text needs to be at MIN 50 characters HELLOOOOO.\"\n",
    "\n",
    "# too lazy to figue our the real way to do this\n",
    "sentence = [encoder[letter] for letter in sentence_raw]\n",
    "\n",
    "x = np.zeros((1,50, len(vocab)))\n",
    "offset = 0\n",
    "print(sentence_raw,end='')\n",
    "for j in range(0,500):\n",
    "    for i,letter in enumerate(sentence[offset:offset+50]):\n",
    "        x[0,i] = onehot[letter]\n",
    "\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    predicted_index = sample(preds, 0.2)\n",
    "    sentence.append(predicted_index)\n",
    "    offset+=1\n",
    "    print(decoder[predicted_index], end=\"\")\n"
   ]
  }
 ],
 "metadata": {
  "colabVersion": "0.1",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
