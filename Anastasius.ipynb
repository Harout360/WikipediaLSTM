{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ðŸ¤´Anastasius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": null
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing import text as Text\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#  Declare  text source and constants\n",
    "It is important to not read the entire file into memory.\n",
    "\n",
    "The file is far too large to keep in memory.\n",
    "\n",
    "Instead we read portions into memory, vectorize it, pass\n",
    "that data into the lstm, and repeat until we process the entire file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": null,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": null
   },
   "outputs": [],
   "source": [
    "# Input sources\n",
    "data_source = 'wikitext-103-raw/wiki.test.vector'\n",
    "vocab_path = 'wikitext-103-raw/wiki.test.vocab'\n",
    "decoder_path = 'wikitext-103-raw/wiki.test.decoder.npy'\n",
    "data_len_path = 'wikitext-103-raw/wiki.test.vector.len'\n",
    "\n",
    "\n",
    "# How many rows to read in from the csv\n",
    "chunk_size = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Load Processed data\n",
    "We need to know all the possible characters in the file. Each of the characters is a class that the network can predict. We find $\\sum$ before doing any heavy operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB:\n",
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~']\n",
      "\n",
      "DECODER:\n",
      "{0: '$', 1: 'o', 2: 'C', 3: 'l', 4: '8', 5: '1', 6: 'd', 7: '\\\\', 8: 'j', 9: \"'\", 10: '+', 11: 'Y', 12: 'f', 13: 'H', 14: ']', 15: 'n', 16: 'L', 17: 'u', 18: '%', 19: '<', 20: 'A', 21: 'i', 22: 'c', 23: 'q', 24: '-', 25: 'h', 26: '0', 27: '2', 28: 'Z', 29: '^', 30: 'Q', 31: ' ', 32: '#', 33: '>', 34: 'a', 35: '3', 36: 'w', 37: 'x', 38: 'I', 39: '\"', 40: 'M', 41: '}', 42: 'v', 43: 'O', 44: 'S', 45: '?', 46: 'r', 47: 'P', 48: ',', 49: 'k', 50: '`', 51: 'T', 52: 'N', 53: 'X', 54: 'y', 55: '4', 56: 'U', 57: 'p', 58: 'g', 59: '\\n', 60: '5', 61: '[', 62: 'm', 63: 't', 64: '/', 65: 'D', 66: '=', 67: '{', 68: '.', 69: '9', 70: 'B', 71: 's', 72: ';', 73: 'V', 74: 'E', 75: 'b', 76: '&', 77: '!', 78: 'G', 79: '6', 80: '~', 81: 'z', 82: 'K', 83: 'e', 84: 'F', 85: 'W', 86: '|', 87: ':', 88: 'J', 89: '_', 90: '(', 91: '7', 92: ')', 93: '*', 94: 'R'}\n",
      "\n",
      "ENCODER:\n",
      "{'w': 36, '+': 10, 'q': 23, 't': 63, 'x': 37, 'E': 74, 'C': 2, 'c': 22, '(': 90, ':': 87, '~': 80, 'V': 73, 'H': 13, '>': 33, '5': 60, '1': 5, '9': 69, 'Y': 11, '\"': 39, ']': 14, '\\n': 59, 'T': 51, \"'\": 9, 'e': 83, 'j': 8, 'Z': 28, ')': 92, 'r': 46, 'P': 47, 'l': 3, '|': 86, 'v': 42, '#': 32, 'W': 85, 'b': 75, '$': 0, 'R': 94, 'O': 43, 'o': 1, '6': 79, 'd': 6, '{': 67, '3': 35, 's': 71, 'g': 58, 'N': 52, '<': 19, '_': 89, 'I': 38, '!': 77, 'G': 78, '^': 29, 'Q': 30, 'L': 16, '\\\\': 7, '2': 27, 'u': 17, 'J': 88, 'y': 54, 'K': 82, 'k': 49, '[': 61, '-': 24, 'D': 65, 'F': 84, 'i': 21, 'B': 70, '`': 50, 'S': 44, 'f': 12, '8': 4, '7': 91, 'p': 57, 'A': 20, '%': 18, 'n': 15, ' ': 31, ';': 72, 'X': 53, '4': 55, 'a': 34, 'U': 56, 'z': 81, '*': 93, '&': 76, '}': 41, 'm': 62, '?': 45, '0': 26, '=': 66, '.': 68, 'M': 40, ',': 48, '/': 64, 'h': 25}\n"
     ]
    }
   ],
   "source": [
    "# Load Vocab\n",
    "with open(vocab_path,\"r\") as file:\n",
    "    vocab = sorted(list(file.read()))\n",
    "\n",
    "# Load Decoder\n",
    "decoder = np.load(decoder_path).item()\n",
    "\n",
    "# Build Encoder\n",
    "encoder = dict((value,key) for key,value in decoder.items())\n",
    "\n",
    "\n",
    "# Load CSV Row count\n",
    "with open(data_len_path,\"r\") as file:\n",
    "    data_row_count = int(file.read())\n",
    "    \n",
    "print(\"VOCAB:\")\n",
    "print(vocab)\n",
    "\n",
    "print(\"\\nDECODER:\")\n",
    "print(decoder)\n",
    "\n",
    "print(\"\\nENCODER:\")\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# One hot encode vocab\n",
    "Passing in raw numbers into an LSTM is apparently a bad idea. We should be passing in one hot encoded values because it does not introduce 'order' or 'weight' for each letter that we don't want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True False False ..., False False False]\n",
      " [False  True False ..., False False False]\n",
      " [False False  True ..., False False False]\n",
      " ..., \n",
      " [False False False ...,  True False False]\n",
      " [False False False ..., False  True False]\n",
      " [False False False ..., False False  True]]\n"
     ]
    }
   ],
   "source": [
    "# Creates one hot encoding from 0.... vocab length\n",
    "onehot = np.eye(len(vocab),dtype=bool)\n",
    "print(onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Construct LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using saved model\n"
     ]
    }
   ],
   "source": [
    "# X Dimension = (Total sentences) x (Sentence Length) x (Character Vector dimension\n",
    "try:\n",
    "    model = load_model(\"ckpt/model.h5py\")\n",
    "    print(\"Using saved model\")\n",
    "except(OSError):\n",
    "    model = Sequential()\n",
    "    input_shape = (50, len(vocab))\n",
    "\n",
    "    model.add(LSTM(64, input_shape=input_shape))\n",
    "    model.add(Dense(len(vocab)))\n",
    "    model.add(Activation('softmax'))\n",
    "    print(\"Using new model\")\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train LSTM\n",
    "We read in slices from the file, vectorize the slice, form our training set, and pass this set into our LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1  of  2\n",
      "Training Model on chunk 1  of  2\n",
      "Epoch 1/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 1.8812Epoch 00000: loss improved from inf to 1.88135, saving model to ckpt/model.h5py\n",
      "100000/100000 [==============================] - 107s - loss: 1.8813   \n",
      "Epoch 2/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 1.7986Epoch 00001: loss improved from 1.88135 to 1.79865, saving model to ckpt/model.h5py\n",
      "100000/100000 [==============================] - 105s - loss: 1.7986   \n",
      "Epoch 3/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 1.7491Epoch 00002: loss improved from 1.79865 to 1.74909, saving model to ckpt/model.h5py\n",
      "100000/100000 [==============================] - 107s - loss: 1.7491   \n",
      "Epoch 4/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 1.7161Epoch 00003: loss improved from 1.74909 to 1.71596, saving model to ckpt/model.h5py\n",
      "100000/100000 [==============================] - 112s - loss: 1.7160   \n",
      "Epoch 5/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 1.6906Epoch 00004: loss improved from 1.71596 to 1.69036, saving model to ckpt/model.h5py\n",
      "100000/100000 [==============================] - 107s - loss: 1.6904   \n",
      "\n",
      "\n",
      "Processing chunk 2  of  2\n",
      "Training Model on chunk 2  of  2\n",
      "Epoch 1/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 0.5059Epoch 00000: loss improved from 1.69036 to 0.50585, saving model to ckpt/model.h5py\n",
      "100000/100000 [==============================] - 99s - loss: 0.5058    \n",
      "Epoch 2/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 0.4779Epoch 00001: loss improved from 0.50585 to 0.47776, saving model to ckpt/model.h5py\n",
      "100000/100000 [==============================] - 93s - loss: 0.4778    \n",
      "Epoch 3/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 0.4639Epoch 00002: loss improved from 0.47776 to 0.46402, saving model to ckpt/model.h5py\n",
      "100000/100000 [==============================] - 91s - loss: 0.4640    \n",
      "Epoch 4/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 0.4528Epoch 00003: loss improved from 0.46402 to 0.45271, saving model to ckpt/model.h5py\n",
      "100000/100000 [==============================] - 92s - loss: 0.4527    \n",
      "Epoch 5/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 0.4439Epoch 00004: loss improved from 0.45271 to 0.44389, saving model to ckpt/model.h5py\n",
      "100000/100000 [==============================] - 91s - loss: 0.4439    \n",
      "\n",
      "\n",
      "Processing chunk 3  of  2\n",
      "Training Model on chunk 3  of  2\n",
      "Epoch 1/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 1.8570Epoch 00000: loss did not improve\n",
      "100000/100000 [==============================] - 92s - loss: 1.8569    \n",
      "Epoch 2/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 1.7333Epoch 00001: loss did not improve\n",
      "100000/100000 [==============================] - 111s - loss: 1.7336   \n",
      "Epoch 3/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 1.6865Epoch 00002: loss did not improve\n",
      "100000/100000 [==============================] - 115s - loss: 1.6865   \n",
      "Epoch 4/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 1.6573Epoch 00003: loss did not improve\n",
      "100000/100000 [==============================] - 106s - loss: 1.6574   \n",
      "Epoch 5/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 1.6345Epoch 00004: loss did not improve\n",
      "100000/100000 [==============================] - 102s - loss: 1.6345   \n",
      "\n",
      "\n",
      "Processing chunk 4  of  2\n",
      "Training Model on chunk 4  of  2\n",
      "Epoch 1/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 0.4797Epoch 00000: loss did not improve\n",
      "100000/100000 [==============================] - 98s - loss: 0.4796    \n",
      "Epoch 2/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 0.4526Epoch 00001: loss did not improve\n",
      "100000/100000 [==============================] - 107s - loss: 0.4526   \n",
      "Epoch 3/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 0.4420Epoch 00002: loss improved from 0.44389 to 0.44205, saving model to ckpt/model.h5py\n",
      "100000/100000 [==============================] - 99s - loss: 0.4421    \n",
      "Epoch 4/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 0.4323Epoch 00003: loss improved from 0.44205 to 0.43242, saving model to ckpt/model.h5py\n",
      "100000/100000 [==============================] - 92s - loss: 0.4324    \n",
      "Epoch 5/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 0.4256Epoch 00004: loss improved from 0.43242 to 0.42569, saving model to ckpt/model.h5py\n",
      "100000/100000 [==============================] - 104s - loss: 0.4257   \n",
      "\n",
      "\n",
      "Processing chunk 5  of  2\n",
      "Training Model on chunk 5  of  2\n",
      "Epoch 1/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 1.8084Epoch 00000: loss did not improve\n",
      "100000/100000 [==============================] - 106s - loss: 1.8084   \n",
      "Epoch 2/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 1.6938Epoch 00001: loss did not improve\n",
      "100000/100000 [==============================] - 100s - loss: 1.6937   \n",
      "Epoch 3/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 1.6539Epoch 00002: loss did not improve\n",
      "100000/100000 [==============================] - 100s - loss: 1.6538   \n",
      "Epoch 4/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 1.6301Epoch 00003: loss did not improve\n",
      "100000/100000 [==============================] - 108s - loss: 1.6301   \n",
      "Epoch 5/5\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 1.6142Epoch 00004: loss did not improve\n",
      "100000/100000 [==============================] - 102s - loss: 1.6142   \n",
      "\n",
      "\n",
      "Processing chunk 6  of  2\n",
      "Training Model on chunk 6  of  2\n",
      "Epoch 1/5\n",
      " 35968/100000 [=========>....................] - ETA: 76s - loss: 0.4685"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-87662b2f2e9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# Train model on X and Y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training Model on chunk\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_processed\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" of \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_chunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mchunk_processed\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\gregl\\Anaconda3\\envs\\intel\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    843\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 845\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    846\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\Users\\gregl\\Anaconda3\\envs\\intel\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1485\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1487\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\gregl\\Anaconda3\\envs\\intel\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1140\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1141\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\gregl\\Anaconda3\\envs\\intel\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2073\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2074\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\gregl\\Anaconda3\\envs\\intel\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\gregl\\Anaconda3\\envs\\intel\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\gregl\\Anaconda3\\envs\\intel\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\gregl\\Anaconda3\\envs\\intel\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\gregl\\Anaconda3\\envs\\intel\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_chunks = math.ceil(data_row_count / chunk_size)\n",
    "chunk_processed = 0\n",
    "\n",
    "# Saves entire model, tracks loss, save after each epoc \n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath=\"ckpt/model.h5py\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    monitor='loss',\n",
    "    mode='auto',\n",
    "    period=1)\n",
    "\n",
    "callbacks = [checkpointer]\n",
    "\n",
    "# Reads in chunk_size amount of rows from csv\n",
    "for file_epocs in range(10):\n",
    "    for chunk in pd.read_csv(data_source, chunksize=chunk_size, sep=';', dtype={'Y':np.int8}):\n",
    "        # Get input vectors ( Convert X to numpy array \n",
    "        print(\"Processing chunk\", chunk_processed + 1,\" of \", total_chunks)\n",
    "        rawX = chunk['X'].map(lambda x: np.fromstring(x, dtype=np.uint8, sep=\" \")).values\n",
    "        rawY = chunk['Y'].values\n",
    "\n",
    "        X = np.zeros(shape=(chunk_size, len(rawX[0]),len(vocab)),dtype=bool)\n",
    "        Y = np.zeros((chunk_size, len(vocab)), dtype=np.bool)\n",
    "\n",
    "        for i in range(len(rawX)):\n",
    "            X[i] = [onehot[value] for value in rawX[i]]\n",
    "            Y[i] = onehot[rawY[i]]\n",
    "\n",
    "        # Train model on X and Y\n",
    "        print(\"Training Model on chunk\", chunk_processed + 1,\" of \", total_chunks)\n",
    "        model.fit(X, Y, batch_size=128, epochs=5, verbose=1,callbacks=callbacks)\n",
    "        print(\"\\n\")\n",
    "        chunk_processed += 1\n",
    "\n",
    "\n",
    "print(\"Model Training Completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "\n",
    "    # helper function to sample an index from a probability array\n",
    "\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "\n",
    "    preds = np.log(preds) / temperature\n",
    "\n",
    "    exp_preds = np.exp(preds)\n",
    "\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, this text needs to be at MIN 50 characters. and a stated the the prossing the reservated the season was the the are in the reservated the surviss of the the the the per the survising the survising the area in the mont and the survising the survics and the the survived . The survics and the commision and the the survising the named the the surviss and the care as the prossing and the pression , and the surviss as the the the the came and the commision , and the area , and the surviss and the rest the for the survicial , and the the prossi"
     ]
    }
   ],
   "source": [
    "sentence_raw = \"Hello, this text needs to be at MIN 50 characters.\"\n",
    "\n",
    "# too lazy to figue our the real way to do this\n",
    "sentence = [encoder[letter] for letter in sentence_raw]\n",
    "\n",
    "x = np.zeros((1,50, len(vocab)))\n",
    "offset = 0\n",
    "print(sentence_raw,end='')\n",
    "for j in range(0,500):\n",
    "    for i,letter in enumerate(sentence[offset:offset+50]):\n",
    "        x[0,i] = onehot[letter]\n",
    "\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    predicted_index = sample(preds, 0.2)\n",
    "    sentence.append(predicted_index)\n",
    "    offset+=1\n",
    "    print(decoder[predicted_index], end=\"\")\n"
   ]
  }
 ],
 "metadata": {
  "colabVersion": "0.1",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
