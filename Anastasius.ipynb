{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤´Anastasius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "executionInfo": null
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing import text as Text\n",
    "from keras.layers import Dropout\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Declare  text source and constants\n",
    "It is important to not read the entire file into memory.\n",
    "\n",
    "The file is far too large to keep in memory.\n",
    "\n",
    "Instead we read portions into memory, vectorize it, pass\n",
    "that data into the lstm, and repeat until we process the entire file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "executionInfo": null
   },
   "outputs": [],
   "source": [
    "# Input sources\n",
    "data_source = 'wikitext-103-raw/wiki.train.vector'\n",
    "vocab_path = 'wikitext-103-raw/wiki.train.vocab'\n",
    "decoder_path = 'wikitext-103-raw/wiki.train.decoder.npy'\n",
    "data_len_path = 'wikitext-103-raw/wiki.train.vector.len'\n",
    "\n",
    "\n",
    "# How many rows to read in from the csv\n",
    "chunk_size = 100000\n",
    "\n",
    "# How many times to run the training\n",
    "numRuns = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Processed data\n",
    "We need to know all the possible characters in the file. Each of the characters is a class that the network can predict. We find $\\sum$ before doing any heavy operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Vocab\n",
    "with open(vocab_path,\"r\") as file:\n",
    "    vocab = sorted(list(file.read()))\n",
    "\n",
    "# Load Decoder\n",
    "decoder = np.load(decoder_path).item()\n",
    "\n",
    "# Build Encoder\n",
    "encoder = dict((value,key) for key,value in decoder.items())\n",
    "\n",
    "\n",
    "# Load CSV Row count\n",
    "with open(data_len_path,\"r\") as file:\n",
    "    data_row_count = int(file.read())\n",
    "    \n",
    "print(\"VOCAB:\")\n",
    "print(vocab)\n",
    "\n",
    "print(\"\\nDECODER:\")\n",
    "print(decoder)\n",
    "\n",
    "print(\"\\nENCODER:\")\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encode vocab\n",
    "Passing in raw numbers into an LSTM is apparently a bad idea. We should be passing in one hot encoded values because it does not introduce 'order' or 'weight' for each letter that we don't want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates one hot encoding from 0.... vocab length\n",
    "onehot = np.eye(len(vocab),dtype=bool)\n",
    "print(onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X Dimension = (Total sentences) x (Sentence Length) x (Character Vector dimension\n",
    "try:\n",
    "    model = load_model(\"ckpt/model.h5py\")\n",
    "    print(\"Using saved model\")\n",
    "except(OSError):\n",
    "    layerSize = 128\n",
    "    input_shape = (50, len(vocab))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=input_shape)))\n",
    "    model.add(Dense(len(vocab)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimizer = RMSprop(lr=0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    print(\"Using new model\")\n",
    "    print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LSTM\n",
    "We read in slices from the file, vectorize the slice, form our training set, and pass this set into our LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_chunks = math.ceil(data_row_count / chunk_size)\n",
    "chunk_processed = 0\n",
    "\n",
    "# Saves entire model, tracks loss, save after each epoc \n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath=\"ckpt/model.h5py\",\n",
    "    verbose=1,\n",
    "    save_best_only=False,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    period=1)\n",
    "\n",
    "callbacks = [checkpointer]\n",
    "\n",
    "\n",
    "# Reads in chunk_size amount of rows from csv\n",
    "for file_epocs in range(numRuns):\n",
    "    for chunk in pd.read_csv(data_source, chunksize=chunk_size, sep=';', dtype={'Y':np.int8}):\n",
    "        # Get input vectors ( Convert X to numpy array \n",
    "        print(\"Processing chunk\", chunk_processed + 1,\" of \", total_chunks * numRuns)\n",
    "        rawX = chunk['X'].map(lambda x: np.fromstring(x, dtype=np.uint8, sep=\" \")).values\n",
    "        rawY = chunk['Y'].values\n",
    "\n",
    "        X = np.zeros(shape=(chunk_size, len(rawX[0]),len(vocab)),dtype=bool)\n",
    "        Y = np.zeros((chunk_size, len(vocab)), dtype=np.bool)\n",
    "\n",
    "        for i in range(len(rawX)):\n",
    "            X[i] = [onehot[value] for value in rawX[i]]\n",
    "            Y[i] = onehot[rawY[i]]\n",
    "\n",
    "        # Train model on X and Y\n",
    "        print(\"Training Model on chunk\", chunk_processed + 1,\" of \", total_chunks * numRuns)\n",
    "        model.fit(X, Y, batch_size=200, epochs=5, verbose=1, callbacks=callbacks)\n",
    "        print(\"\\n\")\n",
    "        chunk_processed += 1\n",
    "\n",
    "\n",
    "print(\"Model Training Completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "\n",
    "    # helper function to sample an index from a probability array\n",
    "\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "\n",
    "    preds = np.log(preds) / temperature\n",
    "\n",
    "    exp_preds = np.exp(preds)\n",
    "\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_raw = \"Hello, this text needs to be at MIN 50 characters HELLO.\"\n",
    "\n",
    "# too lazy to figue our the real way to do this\n",
    "sentence = [encoder[letter] for letter in sentence_raw]\n",
    "\n",
    "x = np.zeros((1,50, len(vocab)))\n",
    "offset = 0\n",
    "print(sentence_raw,end='')\n",
    "for j in range(0,500):\n",
    "    for i,letter in enumerate(sentence[offset:offset+50]):\n",
    "        x[0,i] = onehot[letter]\n",
    "\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    predicted_index = sample(preds, 0.2)\n",
    "    sentence.append(predicted_index)\n",
    "    offset+=1\n",
    "    print(decoder[predicted_index], end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colabVersion": "0.1",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
